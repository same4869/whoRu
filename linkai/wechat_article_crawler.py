#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡çˆ¬å–å™¨
ä»Excelæ–‡ä»¶ä¸­è¯»å–æ–‡ç« é“¾æ¥ï¼Œæ‰¹é‡è·å–æ–‡ç« å†…å®¹å¹¶ä¿å­˜ä¸ºMarkdownæ ¼å¼

åŠŸèƒ½ç‰¹ç‚¹ï¼š
- ä»Excelæ–‡ä»¶è¯»å–æ–‡ç« é“¾æ¥ã€æ ‡é¢˜ã€æ‘˜è¦ã€å‘å¸ƒæ—¶é—´
- æ‰¹é‡è·å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹
- è‡ªåŠ¨ç”Ÿæˆæœ‰åºçš„Markdownæ–‡ä»¶ï¼ˆåºå·+æ—¶é—´+æ ‡é¢˜ï¼‰
- æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé¿å…é‡å¤å¤„ç†
- å¯æ§åˆ¶æ¯æ¬¡å¤„ç†çš„æœ€å¤§è¡Œæ•°
- æ™ºèƒ½é‡è¯•æœºåˆ¶å¤„ç†ç½‘ç»œå¼‚å¸¸

ä½¿ç”¨æ–¹æ³•ï¼š
python wechat_article_crawler.py filename.xlsx           # å¤„ç†æ‰€æœ‰æ–‡ç« 
python wechat_article_crawler.py filename.xlsx --start 10 --count 20  # ä»ç¬¬10è¡Œå¼€å§‹ï¼Œå¤„ç†20ç¯‡æ–‡ç« 
python wechat_article_crawler.py filename.xlsx --max-batch 50         # æ¯æ¬¡æœ€å¤šå¤„ç†50ç¯‡

æ–‡ä»¶ç»“æ„ï¼š
- è¾“å…¥ï¼šwechat/data_setup/filename.xlsx
- è¾“å‡ºï¼šwechat_filename/001_2024-01-01_æ–‡ç« æ ‡é¢˜.md
"""

import os
import sys
import pandas as pd
import requests
import time
import re
import argparse
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse
import html
from bs4 import BeautifulSoup

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

# é…ç½®å‚æ•°
EXCEL_FOLDER = r'../wechat/data_setup'
OUTPUT_BASE_FOLDER = r'.'
DEFAULT_MAX_BATCH = 10000
DEFAULT_DELAY = (2, 5)  # è¯·æ±‚é—´éš”èŒƒå›´ï¼ˆç§’ï¼‰
DEFAULT_TIMEOUT = 30
MAX_RETRIES = 3

# è¯·æ±‚å¤´é…ç½®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

class WeChatArticleCrawler:
    def __init__(self, excel_filename, output_folder=None, delay_range=DEFAULT_DELAY):
        """
        åˆå§‹åŒ–å¾®ä¿¡æ–‡ç« çˆ¬è™«
        
        Args:
            excel_filename: Excelæ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹åï¼ˆå¯é€‰ï¼‰
            delay_range: è¯·æ±‚é—´éš”èŒƒå›´ï¼ˆç§’ï¼‰
        """
        self.excel_filename = excel_filename
        self.excel_path = os.path.join(EXCEL_FOLDER, excel_filename)
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶å¤¹
        if output_folder:
            self.output_folder = output_folder
        else:
            # ä»Excelæ–‡ä»¶åç”Ÿæˆè¾“å‡ºæ–‡ä»¶å¤¹å
            name_without_ext = os.path.splitext(excel_filename)[0]
            self.output_folder = f"wechat_{name_without_ext}"
        
        self.output_path = os.path.join(OUTPUT_BASE_FOLDER, self.output_folder)
        self.delay_range = delay_range
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'success_count': 0,
            'failed_count': 0,
            'skipped_count': 0,
            'start_time': None
        }
        
        print(f"ğŸ“ Excelæ–‡ä»¶: {self.excel_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
    
    def load_excel_data(self):
        """åŠ è½½Excelæ•°æ®"""
        try:
            if not os.path.exists(self.excel_path):
                raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {self.excel_path}")
            
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(self.excel_path)
            print(f"ğŸ“Š åŠ è½½Excelæ•°æ®æˆåŠŸï¼Œå…± {len(df)} è¡Œè®°å½•")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = ['é“¾æ¥', 'æ ‡é¢˜', 'æ‘˜è¦', 'å‘å¸ƒæ—¶é—´']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                print(f"âš ï¸  ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                print(f"ğŸ“‹ å¯ç”¨çš„åˆ—: {list(df.columns)}")
                # å¦‚æœç¼ºå°‘åˆ—ï¼Œå°è¯•ä½¿ç”¨ç›¸ä¼¼çš„åˆ—å
                if 'å‘å¸ƒæ—¶é—´' not in df.columns and 'åˆ›å»ºæ—¶é—´' in df.columns:
                    print("âš ï¸  ä½¿ç”¨'åˆ›å»ºæ—¶é—´'æ›¿ä»£'å‘å¸ƒæ—¶é—´'")
                    df['å‘å¸ƒæ—¶é—´'] = df['åˆ›å»ºæ—¶é—´']
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        filename = filename.replace('\n', ' ').replace('\r', ' ')
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 100:
            filename = filename[:97] + '...'
        return filename.strip()
    
    def format_date(self, date_str):
        """æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²"""
        try:
            if pd.isna(date_str):
                return "æœªçŸ¥æ—¥æœŸ"
            
            # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
            if isinstance(date_str, str):
                # ç§»é™¤å¯èƒ½çš„æ—¶é—´éƒ¨åˆ†ï¼Œåªä¿ç•™æ—¥æœŸ
                date_str = re.sub(r'\s+\d{2}:\d{2}:\d{2}', '', date_str)
                date_str = date_str.strip()
                
                # å°è¯•è§£æ
                try:
                    dt = pd.to_datetime(date_str)
                    return dt.strftime('%Y-%m-%d')
                except:
                    return date_str[:10] if len(date_str) >= 10 else date_str
            elif isinstance(date_str, datetime):
                return date_str.strftime('%Y-%m-%d')
            else:
                return str(date_str)
        except:
            return "æœªçŸ¥æ—¥æœŸ"
    
    def generate_filename(self, index, title, publish_time):
        """ç”Ÿæˆæ–‡ä»¶å"""
        # æ ¼å¼åŒ–åºå·ï¼ˆ3ä½æ•°å­—ï¼‰
        serial = f"{index:03d}"
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        formatted_date = self.format_date(publish_time)
        
        # æ¸…ç†æ ‡é¢˜
        clean_title = self.sanitize_filename(title)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"{serial}_{formatted_date}_{clean_title}.md"
        return filename
    
    def extract_article_content(self, url):
        """æå–æ–‡ç« å†…å®¹"""
        try:
            print(f"    ğŸ“– è·å–æ–‡ç« å†…å®¹...")
            
            response = self.session.get(url, timeout=DEFAULT_TIMEOUT)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« å†…å®¹åŒºåŸŸ
            content_area = soup.find('div', {'class': 'rich_media_content'})
            if not content_area:
                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                content_area = soup.find('div', {'id': 'js_content'})
            
            if content_area:
                # æå–æ–‡æœ¬å†…å®¹
                content = content_area.get_text(separator='\n', strip=True)
                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
                content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
                return content.strip()
            else:
                print(f"    âš ï¸  æœªæ‰¾åˆ°æ–‡ç« å†…å®¹åŒºåŸŸ")
                return None
                
        except requests.RequestException as e:
            print(f"    âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"    âŒ è§£ææ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return None
    
    def create_markdown_content(self, title, summary, publish_time, url, content):
        """åˆ›å»ºMarkdownå†…å®¹"""
        md_content = f"""# {title}

## æ–‡ç« ä¿¡æ¯
- **å‘å¸ƒæ—¶é—´**: {self.format_date(publish_time)}
- **åŸæ–‡é“¾æ¥**: {url}

## æ‘˜è¦
{summary}

## æ­£æ–‡å†…å®¹

{content if content else 'è·å–æ–‡ç« å†…å®¹å¤±è´¥'}

---
*æœ¬æ–‡æ¡£ç”±è‡ªåŠ¨åŒ–å·¥å…·ç”Ÿæˆ*
"""
        return md_content
    
    def save_article(self, filename, title, summary, publish_time, url, content):
        """ä¿å­˜æ–‡ç« ä¸ºMarkdownæ–‡ä»¶"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(self.output_path, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            file_path = os.path.join(self.output_path, filename)
            
            # åˆ›å»ºMarkdownå†…å®¹
            md_content = self.create_markdown_content(title, summary, publish_time, url, content)
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            print(f"    âœ… å·²ä¿å­˜: {filename}")
            return True
            
        except Exception as e:
            print(f"    âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def process_article(self, index, row):
        """å¤„ç†å•ç¯‡æ–‡ç« """
        try:
            url = row.get('é“¾æ¥', '')
            title = row.get('æ ‡é¢˜', f'æ–‡ç« {index}')
            summary = row.get('æ‘˜è¦', 'æš‚æ— æ‘˜è¦')
            publish_time = row.get('å‘å¸ƒæ—¶é—´', '')
            
            print(f"\nğŸ“„ å¤„ç†ç¬¬ {index} ç¯‡æ–‡ç« : {title[:50]}...")
            
            # æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ
            if not url or pd.isna(url):
                print(f"    âš ï¸  é“¾æ¥ä¸ºç©ºï¼Œè·³è¿‡")
                return 'skipped'
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = self.generate_filename(index, title, publish_time)
            file_path = os.path.join(self.output_path, filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            if os.path.exists(file_path):
                print(f"    âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return 'skipped'
            
            # æå–æ–‡ç« å†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰
            content = None
            for attempt in range(MAX_RETRIES):
                try:
                    content = self.extract_article_content(url)
                    if content:
                        break
                    elif attempt < MAX_RETRIES - 1:
                        print(f"    ğŸ”„ ç¬¬ {attempt + 1} æ¬¡é‡è¯•...")
                        time.sleep(2)
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        print(f"    ğŸ”„ é‡è¯•ä¸­... ({attempt + 1}/{MAX_RETRIES})")
                        time.sleep(2)
                    else:
                        print(f"    âŒ é‡è¯•å¤±è´¥: {e}")
            
            # ä¿å­˜æ–‡ç« 
            if self.save_article(filename, title, summary, publish_time, url, content):
                return 'success'
            else:
                return 'failed'
                
        except Exception as e:
            print(f"    âŒ å¤„ç†æ–‡ç« å¤±è´¥: {e}")
            return 'failed'
    
    def get_random_delay(self):
        """è·å–éšæœºå»¶æ—¶"""
        import random
        return random.uniform(self.delay_range[0], self.delay_range[1])
    
    def process_articles(self, start_index=1, max_count=None, max_batch=DEFAULT_MAX_BATCH):
        """æ‰¹é‡å¤„ç†æ–‡ç« """
        # åŠ è½½æ•°æ®
        df = self.load_excel_data()
        if df is None:
            return False
        
        # è®¡ç®—å¤„ç†èŒƒå›´
        total_articles = len(df)
        if start_index > total_articles:
            print(f"âŒ èµ·å§‹ä½ç½® {start_index} è¶…å‡ºæ€»è®°å½•æ•° {total_articles}")
            return False
        
        # è®¡ç®—å®é™…å¤„ç†æ•°é‡
        end_index = start_index + (max_count if max_count else total_articles) - 1
        end_index = min(end_index, total_articles)
        end_index = min(end_index, start_index + max_batch - 1)
        
        process_count = end_index - start_index + 1
        
        print(f"\nğŸ“‹ å¤„ç†è®¡åˆ’:")
        print(f"   æ€»è®°å½•æ•°: {total_articles}")
        print(f"   å¤„ç†èŒƒå›´: ç¬¬ {start_index} - {end_index} è¡Œ")
        print(f"   å¤„ç†æ•°é‡: {process_count} ç¯‡æ–‡ç« ")
        print(f"   è¾“å‡ºç›®å½•: {self.output_path}")
        
        # å¼€å§‹å¤„ç†
        self.stats['start_time'] = datetime.now()
        
        for i in range(start_index - 1, end_index):  # è½¬æ¢ä¸º0åŸºç´¢å¼•
            row = df.iloc[i]
            result = self.process_article(i + 1, row)  # æ˜¾ç¤ºä¸º1åŸºç´¢å¼•
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_processed'] += 1
            if result == 'success':
                self.stats['success_count'] += 1
            elif result == 'failed':
                self.stats['failed_count'] += 1
            elif result == 'skipped':
                self.stats['skipped_count'] += 1
            
            # å»¶æ—¶é¿å…é¢‘ç¹è¯·æ±‚
            if i < end_index - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶æ—¶
                delay = self.get_random_delay()
                print(f"    â° ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        self.print_statistics()
        return True
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        end_time = datetime.now()
        elapsed_time = end_time - self.stats['start_time']
        
        print(f"\n" + "=" * 50)
        print(f"ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
        print(f"=" * 50)
        print(f"â° å¼€å§‹æ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time}")
        print(f"ğŸ“ˆ æ€»å¤„ç†: {self.stats['total_processed']} ç¯‡")
        print(f"âœ… æˆåŠŸ: {self.stats['success_count']} ç¯‡")
        print(f"â­ï¸  è·³è¿‡: {self.stats['skipped_count']} ç¯‡")
        print(f"âŒ å¤±è´¥: {self.stats['failed_count']} ç¯‡")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
        print(f"=" * 50)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡çˆ¬å–å™¨')
    parser.add_argument('excel_file', help='Excelæ–‡ä»¶åï¼ˆåœ¨wechat/data_setupç›®å½•ä¸‹ï¼‰')
    parser.add_argument('--start', type=int, default=1, help='èµ·å§‹è¡Œå·ï¼ˆé»˜è®¤: 1ï¼‰')
    parser.add_argument('--count', type=int, help='å¤„ç†æ–‡ç« æ•°é‡ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰')
    parser.add_argument('--max-batch', type=int, default=DEFAULT_MAX_BATCH, 
                       help=f'æ¯æ¬¡æœ€å¤§å¤„ç†æ•°é‡ï¼ˆé»˜è®¤: {DEFAULT_MAX_BATCH}ï¼‰')
    parser.add_argument('--output', help='è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶å¤¹å')
    parser.add_argument('--delay', type=str, default='2,5', 
                       help='è¯·æ±‚å»¶æ—¶èŒƒå›´ï¼ˆç§’ï¼‰ï¼Œæ ¼å¼: min,maxï¼ˆé»˜è®¤: 2,5ï¼‰')
    
    args = parser.parse_args()
    
    # è§£æå»¶æ—¶å‚æ•°
    try:
        delay_min, delay_max = map(float, args.delay.split(','))
        delay_range = (delay_min, delay_max)
    except:
        delay_range = DEFAULT_DELAY
        print(f"âš ï¸  å»¶æ—¶å‚æ•°æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤å€¼: {DEFAULT_DELAY}")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WeChatArticleCrawler(
        excel_filename=args.excel_file,
        output_folder=args.output,
        delay_range=delay_range
    )
    
    # å¼€å§‹å¤„ç†
    try:
        success = crawler.process_articles(
            start_index=args.start,
            max_count=args.count,
            max_batch=args.max_batch
        )
        
        if success:
            print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
        else:
            print("\nâŒ å¤„ç†å¤±è´¥ï¼")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
